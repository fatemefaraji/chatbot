{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RA5pV2sI73ol",
        "outputId": "3d068366-71d0-45ff-b6a5-0c64adb939ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDI6krFS7EUr",
        "outputId": "fa38f79b-e1ef-4ce7-d4ab-56ef1ce36980"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys in police_db.json:\n",
            "{'address', 'phone', 'id', 'name'}\n",
            "--------------------------------------------------\n",
            "Keys in slot_descriptions.json:\n",
            "{'restaurant-area', 'attraction-area', 'train-day', 'restaurant-book people', 'taxi-leaveat', 'bus-day', 'hotel-book people', 'hospital-department', 'restaurant-name', 'train-departure', 'hotel-internet', 'bus-arriveBy', 'hotel-stars', 'bus-departure', 'train-book people', 'taxi-arriveby', 'attraction-type', 'train-arriveby', 'train-leaveat', 'attraction-name', 'hotel-parking', 'hotel-name', 'taxi-departure', 'hotel-pricerange', 'bus-people', 'hotel-book stay', 'restaurant-food', 'train-destination', 'restaurant-book day', 'restaurant-pricerange', 'hotel-book day', 'bus-destination', 'hotel-type', 'taxi-destination', 'bus-leaveAt', 'hotel-area', 'restaurant-book time'}\n",
            "--------------------------------------------------\n",
            "Keys in ontology.json:\n",
            "{'train-semi-arriveBy', 'hotel-semi-parking', 'attraction-semi-area', 'train-book-people', 'restaurant-semi-food', 'restaurant-book-day', 'attraction-semi-name', 'bus-semi-day', 'hotel-book-people', 'hotel-semi-internet', 'restaurant-semi-pricerange', 'train-semi-day', 'attraction-semi-type', 'bus-semi-departure', 'hotel-semi-pricerange', 'restaurant-book-time', 'taxi-semi-leaveAt', 'train-semi-destination', 'taxi-semi-destination', 'taxi-semi-departure', 'taxi-semi-arriveBy', 'hotel-book-stay', 'hotel-semi-area', 'bus-semi-leaveAt', 'bus-semi-destination', 'train-semi-departure', 'restaurant-semi-area', 'restaurant-semi-name', 'hotel-semi-name', 'hospital-semi-department', 'hotel-book-day', 'hotel-semi-stars', 'train-semi-leaveAt', 'hotel-semi-type', 'restaurant-book-people'}\n",
            "--------------------------------------------------\n",
            "Keys in attraction_db.json:\n",
            "{'pricerange', 'id', 'name', 'entrance fee', 'openhours', 'location', 'address', 'phone', 'postcode', 'type', 'area'}\n",
            "--------------------------------------------------\n",
            "Keys in hospital_db.json:\n",
            "{'phone', 'id', 'department'}\n",
            "--------------------------------------------------\n",
            "Keys in hotel_db.json:\n",
            "{'price', 'parking', 'single', 'pricerange', 'id', 'stars', 'name', 'internet', 'location', 'family', 'address', 'takesbookings', 'double', 'phone', 'n', 'postcode', 'type', 'area'}\n",
            "--------------------------------------------------\n",
            "Keys in restaurant_db.json:\n",
            "{'pricerange', 'id', 'name', 'food', 'signature', 'location', 'address', 'introduction', 'phone', 'postcode', 'type', 'area'}\n",
            "--------------------------------------------------\n",
            "Keys in taxi_db.json:\n",
            "{'taxi_phone', 'taxi_colors', 'taxi_types'}\n",
            "--------------------------------------------------\n",
            "Keys in train_db.json:\n",
            "{'price', 'trainID', 'arriveBy', 'duration', 'departure', 'destination', 'leaveAt', 'day'}\n",
            "--------------------------------------------------\n",
            " processed database data saved to '/content/drive/MyDrive/Week 4/data/processed_database.json'.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "data_path = \"/content/drive/MyDrive/Week 4/data\"\n",
        "dataset_files = [\n",
        "    \"police_db.json\",\n",
        "    \"slot_descriptions.json\",\n",
        "    \"ontology.json\",\n",
        "    \"attraction_db.json\",\n",
        "    \"hospital_db.json\",\n",
        "    \"hotel_db.json\",\n",
        "    \"restaurant_db.json\",\n",
        "    \"taxi_db.json\",\n",
        "    \"train_db.json\"\n",
        "]\n",
        "\n",
        "\n",
        "structured_data = {}\n",
        "\n",
        "def load_data(file_name):\n",
        "    file_path = os.path.join(data_path, file_name)\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
        "            return json.load(file)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"file not found: {file_path}\")\n",
        "        return None\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"error decoding JSON from file {file_path}: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\" error occurred while processing file {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def extract_keys(data):\n",
        "    keys = set()\n",
        "    if isinstance(data, dict):\n",
        "        for key, value in data.items():\n",
        "            keys.add(key)\n",
        "            keys.update(extract_keys(value))\n",
        "    elif isinstance(data, list):\n",
        "        for item in data:\n",
        "            keys.update(extract_keys(item))\n",
        "    return keys\n",
        "\n",
        "for file_name in dataset_files:\n",
        "    data = load_data(file_name)\n",
        "\n",
        "    if data is not None:\n",
        "        keys = extract_keys(data)\n",
        "        print(f\"Keys in {file_name}:\")\n",
        "        print(keys)\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        domain = file_name.replace(\"_db.json\", \"\").replace(\".json\", \"\")\n",
        "\n",
        "        if \"_db.json\" in file_name:\n",
        "            structured_data[domain] = data\n",
        "\n",
        "output_file = os.path.join(data_path, \"processed_database.json\")\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
        "    json.dump(structured_data, file, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(f\" processed database data saved to '{output_file}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TArTRLLAz4m",
        "outputId": "e8f8f93a-7c58-44bb-a5a4-6846e71d7429"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conversational slots processed and saved to '/content/drive/MyDrive/Week 4/data/processed_slots.json'.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "data_path = \"/content/drive/MyDrive/Week 4/data\"\n",
        "slot_file = os.path.join(data_path, \"slot_descriptions.json\")\n",
        "\n",
        "with open(slot_file, \"r\", encoding=\"utf-8\") as file:\n",
        "    slot_data = json.load(file)\n",
        "\n",
        "slot_dict = {}\n",
        "\n",
        "for slot in slot_data.keys():\n",
        "    domain = slot.split(\"-\")[0]\n",
        "    if domain not in slot_dict:\n",
        "        slot_dict[domain] = []\n",
        "    slot_dict[domain].append(slot)\n",
        "\n",
        "output_slot_file = os.path.join(data_path, \"processed_slots.json\")\n",
        "with open(output_slot_file, \"w\", encoding=\"utf-8\") as file:\n",
        "    json.dump(slot_dict, file, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(f\"conversational slots processed and saved to '{output_slot_file}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYmmE_O_BHLh",
        "outputId": "9d45e261-7f21-4e02-b5a9-732cf059e9ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ontology values processed and saved to '/content/drive/MyDrive/Week 4/data/processed_ontology.json'.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "data_path = \"/content/drive/MyDrive/Week 4/data\"\n",
        "ontology_file = os.path.join(data_path, \"ontology.json\")\n",
        "\n",
        "with open(ontology_file, \"r\", encoding=\"utf-8\") as file:\n",
        "    ontology_data = json.load(file)\n",
        "\n",
        "ontology_dict = {}\n",
        "\n",
        "for slot, values in ontology_data.items():\n",
        "    domain = slot.split(\"-\")[0]\n",
        "    if domain not in ontology_dict:\n",
        "        ontology_dict[domain] = {}\n",
        "\n",
        "    ontology_dict[domain][slot] = values\n",
        "output_ontology_file = os.path.join(data_path, \"processed_ontology.json\")\n",
        "with open(output_ontology_file, \"w\", encoding=\"utf-8\") as file:\n",
        "    json.dump(ontology_dict, file, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(f\"ontology values processed and saved to '{output_ontology_file}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeohS8BOBZ4c",
        "outputId": "12671b01-4df6-4018-b2cd-77aaf6e60204"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chatbot training data prepared and saved to '/content/drive/MyDrive/Week 4/data/chatbot_training_data.json'!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "data_path = \"/content/drive/MyDrive/Week 4/data\"\n",
        "\n",
        "with open(os.path.join(data_path, \"processed_database.json\"), \"r\", encoding=\"utf-8\") as file:\n",
        "    database = json.load(file)\n",
        "\n",
        "with open(os.path.join(data_path, \"processed_slots.json\"), \"r\", encoding=\"utf-8\") as file:\n",
        "    slots = json.load(file)\n",
        "\n",
        "with open(os.path.join(data_path, \"processed_ontology.json\"), \"r\", encoding=\"utf-8\") as file:\n",
        "    ontology = json.load(file)\n",
        "\n",
        "training_data = []\n",
        "\n",
        "for domain, entries in database.items():\n",
        "    if domain in slots:\n",
        "        for entry in entries:\n",
        "            for slot in slots[domain]:\n",
        "                slot_key = slot.replace(domain + \"-\", \"\")\n",
        "\n",
        "                if slot_key in entry:\n",
        "                    question = f\"What is the {slot_key} of the {domain}?\"\n",
        "                    answer = str(entry[slot_key])\n",
        "\n",
        "                    training_data.append({\"input\": question, \"output\": answer})\n",
        "\n",
        "output_file = os.path.join(data_path, \"chatbot_training_data.json\")\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
        "    json.dump(training_data, file, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(f\"chatbot training data prepared and saved to '{output_file}'!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdGz0g9HBZ02",
        "outputId": "f1cec330-daff-40b0-c60f-d6a98a935bf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary saved: /content/drive/MyDrive/Week 4/data/vocab.json\n",
            "tokenized data saved: /content/drive/MyDrive/Week 4/data/tokenized_training_data.json\n",
            "[{'input_ids': [3, 6, 12, 9, 3, 8, 4, 11, 50, 83, 3, 7, 4, 3, 14, 63, 68, 5], 'output_ids': [3, 19, 113, 170, 53, 84]}, {'input_ids': [3, 6, 12, 9, 3, 8, 4, 70, 3, 7, 4, 3, 14, 63, 68, 5], 'output_ids': [3, 157, 9]}, {'input_ids': [3, 6, 12, 9, 3, 8, 4, 3, 71, 3, 7, 4, 3, 14, 63, 68, 5], 'output_ids': [145, 151, 151, 94, 50, 3, 84, 3, 128, 3, 465, 46, 108, 185, 9, 112]}, {'input_ids': [3, 6, 12, 9, 3, 8, 4, 11, 50, 83, 3, 7, 4, 3, 14, 63, 68, 5], 'output_ids': [184]}, {'input_ids': [3, 6, 12, 9, 3, 8, 4, 70, 3, 7, 4, 3, 14, 63, 68, 5], 'output_ids': [95]}]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import sentencepiece as spm\n",
        "import numpy as np\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/Week 4/data\"\n",
        "tokenizer_model_prefix = os.path.join(data_path, \"tokenizer\")\n",
        "training_data_file = os.path.join(data_path, \"chatbot_training_data.json\")\n",
        "\n",
        "with open(training_data_file, \"r\", encoding=\"utf-8\") as file:\n",
        "    training_data = json.load(file)\n",
        "\n",
        "text_corpus = [item[\"input\"] for item in training_data] + [item[\"output\"] for item in training_data]\n",
        "\n",
        "corpus_file = os.path.join(data_path, \"corpus.txt\")\n",
        "with open(corpus_file, \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(\"\\n\".join(text_corpus))\n",
        "\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input=corpus_file,\n",
        "    model_prefix=tokenizer_model_prefix,\n",
        "    vocab_size=500\n",
        ")\n",
        "sp = spm.SentencePieceProcessor(model_file=f\"{tokenizer_model_prefix}.model\")\n",
        "\n",
        "vocab = {sp.id_to_piece(i): i for i in range(sp.get_piece_size())}\n",
        "\n",
        "vocab_file = os.path.join(data_path, \"vocab.json\")\n",
        "with open(vocab_file, \"w\", encoding=\"utf-8\") as file:\n",
        "    json.dump(vocab, file, indent=4)\n",
        "\n",
        "print(f\"vocabulary saved: {vocab_file}\")\n",
        "tokenized_data = [\n",
        "    {\n",
        "        \"input_ids\": sp.encode(item[\"input\"], out_type=int),\n",
        "        \"output_ids\": sp.encode(item[\"output\"], out_type=int)\n",
        "    }\n",
        "    for item in training_data\n",
        "]\n",
        "\n",
        "tokenized_data_file = os.path.join(data_path, \"tokenized_training_data.json\")\n",
        "with open(tokenized_data_file, \"w\", encoding=\"utf-8\") as file:\n",
        "    json.dump(tokenized_data, file, indent=4)\n",
        "\n",
        "print(f\"tokenized data saved: {tokenized_data_file}\")\n",
        "print(tokenized_data[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 128\n",
        "BATCH_SIZE = 32\n",
        "BUFFER_SIZE = 10000\n",
        "VOCAB_SIZE = 5000\n",
        "D_MODEL = 512\n",
        "DROPOUT_RATE = 0.1\n",
        "DFF = 2048\n",
        "NUM_HEADS = 8\n",
        "NUM_LAYERS = 6\n",
        "\n",
        "class TokenEmbedding(layers.Layer):\n",
        "    def __init__(self, vocab_size, d_model, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.emb = layers.Embedding(input_dim=vocab_size, output_dim=d_model)\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.emb(x)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"vocab_size\": self.vocab_size,\n",
        "            \"d_model\": self.d_model\n",
        "        })\n",
        "        return config\n",
        "\n",
        "class LSHSelfAttention(layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.wq = layers.Dense(d_model // 2)\n",
        "        self.wk = layers.Dense(d_model // 2)\n",
        "        self.wv = layers.Dense(d_model // 2)\n",
        "        self.dense = layers.Dense(d_model // 2)\n",
        "\n",
        "    def call(self, x):\n",
        "        q, k, v = tf.split(x, num_or_size_splits=3, axis=-1)\n",
        "        q, k, v = self.wq(q), self.wk(k), self.wv(v)\n",
        "        attention_weights = tf.nn.softmax(tf.matmul(q, k, transpose_b=True) / tf.sqrt(tf.cast(tf.shape(k)[-1], tf.float32)), axis=-1)\n",
        "        return self.dense(tf.matmul(attention_weights, v))\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"d_model\": self.d_model,\n",
        "            \"num_heads\": self.num_heads\n",
        "        })\n",
        "        return config\n",
        "\n",
        "class FeedForward(layers.Layer):\n",
        "    def __init__(self, d_model, dff, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.d_model = d_model\n",
        "        self.dff = dff\n",
        "        self.dense1 = layers.Dense(dff, activation='gelu')\n",
        "        self.dense2 = layers.Dense(d_model // 2)\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.dense2(self.dense1(x))\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"d_model\": self.d_model,\n",
        "            \"dff\": self.dff\n",
        "        })\n",
        "        return config\n",
        "\n",
        "class ReversibleResidualLayer(layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.dff = dff\n",
        "        self.f = LSHSelfAttention(d_model, num_heads)\n",
        "        self.g = FeedForward(d_model, dff)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x1, x2 = tf.split(inputs, num_or_size_splits=2, axis=-1)\n",
        "        y1 = x1 + self.f(tf.concat([x2, x2, x2], axis=-1))\n",
        "        y2 = x2 + self.g(y1)\n",
        "        return tf.concat([y1, y2], axis=-1)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"d_model\": self.d_model,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dff\": self.dff\n",
        "        })\n",
        "        return config\n",
        "\n",
        "class ReformerBlock(layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.dff = dff\n",
        "        self.residual = ReversibleResidualLayer(d_model, num_heads, dff)\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.residual(x)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"d_model\": self.d_model,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dff\": self.dff\n",
        "        })\n",
        "        return config\n",
        "\n",
        "class Reformer(keras.Model):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, dff, num_layers, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.dff = dff\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = TokenEmbedding(vocab_size, d_model)\n",
        "        self.reformer_blocks = [ReformerBlock(d_model, num_heads, dff)\n",
        "                              for _ in range(num_layers)]\n",
        "        self.final_layer = layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embedding(x)\n",
        "        for layer in self.reformer_blocks:\n",
        "            x = layer(x)\n",
        "        return self.final_layer(x)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"vocab_size\": self.vocab_size,\n",
        "            \"d_model\": self.d_model,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dff\": self.dff,\n",
        "            \"num_layers\": self.num_layers\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/Week 4/data/tokenized_training_data.json\"\n",
        "with open(data_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    tokenized_data = json.load(file)\n",
        "\n",
        "input_sequences = [item['input_ids'] for item in tokenized_data]\n",
        "target_sequences = [item['output_ids'] for item in tokenized_data]\n",
        "\n",
        "input_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    input_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\")\n",
        "target_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    target_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\")\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_sequences, target_sequences))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "reformer = Reformer(VOCAB_SIZE, D_MODEL, NUM_HEADS, DFF, NUM_LAYERS)\n",
        "reformer.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=3e-4),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = reformer.fit(\n",
        "    dataset,\n",
        "    epochs=10,\n",
        "    callbacks=[\n",
        "        keras.callbacks.ModelCheckpoint(\n",
        "            'reformer_checkpoint_{epoch}.keras',\n",
        "            save_best_only=True,\n",
        "            monitor='loss'\n",
        "        ),\n",
        "        keras.callbacks.EarlyStopping(\n",
        "            monitor='loss',\n",
        "            patience=3,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "reformer.save('reformer_final_model.keras')\n",
        "\n",
        "custom_objects = {\n",
        "    \"Reformer\": Reformer,\n",
        "    \"ReformerBlock\": ReformerBlock,\n",
        "    \"ReversibleResidualLayer\": ReversibleResidualLayer,\n",
        "    \"LSHSelfAttention\": LSHSelfAttention,\n",
        "    \"FeedForward\": FeedForward,\n",
        "    \"TokenEmbedding\": TokenEmbedding\n",
        "}\n",
        "\n",
        "\n",
        "try:\n",
        "    loaded_model = keras.models.load_model('reformer_final_model.keras',\n",
        "                                         custom_objects=custom_objects)\n",
        "    print(\"model successfully saved and loaded!\")\n",
        "except Exception as e:\n",
        "    print(f\"rrror loading model: {e}\")"
      ],
      "metadata": {
        "id": "dfNBcrJqjhYH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce2dfa07-33a3-4ee5-828e-58c4c9580e0d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function ConcreteFunctionGarbageCollector.__del__ at 0x7f13960b19e0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\", line 1749, in __del__\n",
            "    def __del__(self):\n",
            "  \n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 116ms/step - accuracy: 0.9553 - loss: 0.6376\n",
            "Epoch 2/10\n",
            "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 102ms/step - accuracy: 0.9758 - loss: 0.1190\n",
            "Epoch 3/10\n",
            "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 102ms/step - accuracy: 0.9766 - loss: 0.1065\n",
            "Epoch 4/10\n",
            "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 103ms/step - accuracy: 0.9782 - loss: 0.0898\n",
            "Epoch 5/10\n",
            "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 102ms/step - accuracy: 0.9778 - loss: 0.0890\n",
            "Epoch 6/10\n",
            "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 102ms/step - accuracy: 0.9779 - loss: 0.0873\n",
            "Epoch 7/10\n",
            "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 102ms/step - accuracy: 0.9779 - loss: 0.0866\n",
            "Epoch 8/10\n",
            "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 102ms/step - accuracy: 0.9781 - loss: 0.0853\n",
            "Epoch 9/10\n",
            "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 102ms/step - accuracy: 0.9778 - loss: 0.0857\n",
            "Epoch 10/10\n",
            "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 102ms/step - accuracy: 0.9783 - loss: 0.0838\n",
            "Model successfully saved and loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"Model saved at:\", os.path.abspath(\"reformer_final_model.h5\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2so8GUAkFQdq",
        "outputId": "55a7354f-0783-4b59-f572-435d9a0613c3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved at: /content/reformer_final_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir(\"/content\"))\n"
      ],
      "metadata": {
        "id": "vkeq6xCwF7f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57a39f7c-ae13-47d9-bd0a-823336f76bb4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'reformer_checkpoint_7.h5', 'reformer_checkpoint_10.h5', 'reformer_checkpoint_2.h5', 'reformer_checkpoint_9.h5', 'reformer_checkpoint_6.h5', 'reformer_checkpoint_3.h5', 'reformer_checkpoint_4.h5', 'reformer_final_model.h5', 'reformer_checkpoint_1.h5', 'drive', 'reformer_checkpoint_5.h5', 'reformer_checkpoint_8.h5', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reformer.save('reformer_final_model.keras')\n"
      ],
      "metadata": {
        "id": "gZWDtToBQq0q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import sentencepiece as spm\n",
        "import json\n",
        "import os\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "class DatabaseManager:\n",
        "    def __init__(self, data_path: str):\n",
        "        with open(os.path.join(data_path, \"processed_slots.json\"), \"r\", encoding=\"utf-8\") as f:\n",
        "            self.slots = json.load(f)\n",
        "        with open(os.path.join(data_path, \"processed_ontology.json\"), \"r\", encoding=\"utf-8\") as f:\n",
        "            self.ontology = json.load(f)\n",
        "        with open(os.path.join(data_path, \"processed_database.json\"), \"r\", encoding=\"utf-8\") as f:\n",
        "            self.database = json.load(f)\n",
        "\n",
        "    def extract_slots(self, text: str) -> Dict[str, str]:\n",
        "        slots_found = {}\n",
        "        text = text.lower()\n",
        "\n",
        "        categories = ['hotel', 'restaurant', 'train', 'attraction', 'taxi', 'bus', 'hospital']\n",
        "        category = next((cat for cat in categories if cat in text), None)\n",
        "\n",
        "        if category:\n",
        "            for slot in self.slots.get(category, []):\n",
        "                slot_values = self.ontology.get(category, {}).get(f\"{category}-semi-{slot.split('-')[-1]}\", [])\n",
        "                for value in slot_values:\n",
        "                    if value.lower() in text:\n",
        "                        slots_found[slot] = value\n",
        "\n",
        "        return slots_found\n",
        "\n",
        "    def query_database(self, category: str, slots: Dict[str, str]) -> List[Dict]:\n",
        "        results = []\n",
        "        entries = self.database.get(category, [])\n",
        "\n",
        "        for entry in entries:\n",
        "            matches = True\n",
        "            for slot, value in slots.items():\n",
        "                if slot.split('-')[-1] in entry:\n",
        "                    if entry[slot.split('-')[-1]].lower() != value.lower():\n",
        "                        matches = False\n",
        "                        break\n",
        "            if matches:\n",
        "                results.append(entry)\n",
        "\n",
        "        return results\n",
        "\n",
        "class EnhancedChatbot:\n",
        "    def __init__(self, model_path: str, tokenizer_path: str, data_path: str):\n",
        "        self.max_sequence_length = 128\n",
        "\n",
        "        self.model = keras.models.load_model(model_path, custom_objects={\n",
        "            \"Reformer\": Reformer,\n",
        "            \"ReformerBlock\": ReformerBlock,\n",
        "            \"ReversibleResidualLayer\": ReversibleResidualLayer,\n",
        "            \"LSHSelfAttention\": LSHSelfAttention,\n",
        "            \"FeedForward\": FeedForward,\n",
        "            \"TokenEmbedding\": TokenEmbedding\n",
        "        })\n",
        "\n",
        "        self.tokenizer = spm.SentencePieceProcessor()\n",
        "        self.tokenizer.load(tokenizer_path)\n",
        "        self.db_manager = DatabaseManager(data_path)\n",
        "\n",
        "    def preprocess_input(self, text: str) -> np.ndarray:\n",
        "        \"\"\"Preprocess input text for the model.\"\"\"\n",
        "        tokens = self.tokenizer.encode_as_ids(text)\n",
        "        if len(tokens) > self.max_sequence_length:\n",
        "            tokens = tokens[:self.max_sequence_length]\n",
        "        padded_tokens = np.zeros((1, self.max_sequence_length), dtype=np.int32)\n",
        "        padded_tokens[0, :len(tokens)] = tokens\n",
        "        return padded_tokens\n",
        "\n",
        "    def postprocess_output(self, predictions: np.ndarray) -> str:\n",
        "        predicted_ids = np.argmax(predictions[0], axis=-1)\n",
        "        if self.tokenizer.eos_id() in predicted_ids:\n",
        "            predicted_ids = predicted_ids[:np.where(predicted_ids == self.tokenizer.eos_id())[0][0]]\n",
        "        return self.tokenizer.decode_ids(predicted_ids.tolist())\n",
        "\n",
        "    def format_database_response(self, category: str, results: List[Dict]) -> str:\n",
        "        if not results:\n",
        "            return f\"I couldn't find any {category} matching your criteria.\"\n",
        "\n",
        "        response = f\"I found {len(results)} matching {category}(s):\\n\"\n",
        "        for i, result in enumerate(results, 1):\n",
        "            response += f\"\\n{i}. \"\n",
        "            if 'name' in result:\n",
        "                response += f\"{result['name']}\"\n",
        "            if 'type' in result:\n",
        "                response += f\" ({result['type']})\"\n",
        "            if 'area' in result:\n",
        "                response += f\" in {result['area']}\"\n",
        "            if 'pricerange' in result:\n",
        "                response += f\" - {result['pricerange']} price range\"\n",
        "            if 'phone' in result:\n",
        "                response += f\"\\n   phone: {result['phone']}\"\n",
        "            if 'address' in result:\n",
        "                response += f\"\\n   address: {result['address']}\"\n",
        "\n",
        "        return response\n",
        "\n",
        "    def handle_query(self, user_input: str) -> str:\n",
        "        slots = self.db_manager.extract_slots(user_input)\n",
        "\n",
        "        if slots:\n",
        "            category = next((cat for cat in self.db_manager.slots.keys()\n",
        "                           if any(slot.startswith(cat) for slot in slots)), None)\n",
        "            if category:\n",
        "                results = self.db_manager.query_database(category, slots)\n",
        "                if results:\n",
        "                    return self.format_database_response(category, results)\n",
        "\n",
        "        try:\n",
        "            preprocessed_input = self.preprocess_input(user_input)\n",
        "            print(f\"preprocessed input: {preprocessed_input}\")\n",
        "            with tf.device('/CPU:0'):\n",
        "                prediction = self.model.predict(preprocessed_input, verbose=0)\n",
        "            print(f\"model prediction: {prediction}\")\n",
        "            return self.postprocess_output(prediction)\n",
        "        except Exception as e:\n",
        "            return f\"I apologize, but Im facing an error: {str(e)}\"\n",
        "\n",
        "    def chat(self):\n",
        "        print(\"welcome! Type 'exit' to end the chat.\")\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                user_input = input(\"You: \").strip()\n",
        "\n",
        "                if user_input.lower() == 'exit':\n",
        "                    print(\"Goodbye! have a nice day !\")\n",
        "                    break\n",
        "\n",
        "                if not user_input:\n",
        "                    print(\"please type something!\")\n",
        "                    continue\n",
        "\n",
        "                response = self.handle_query(user_input)\n",
        "                print(f\"assistant: {response}\")\n",
        "\n",
        "            except KeyboardInterrupt:\n",
        "                print(\"\\nChat ended by user.\")\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"error occurred: {str(e)}\")\n",
        "                print(\"try again!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    MODEL_PATH = 'reformer_final_model.keras'\n",
        "    TOKENIZER_PATH = \"/content/drive/MyDrive/Week 4/data/tokenizer.model\"\n",
        "    DATA_PATH = \"/content/drive/MyDrive/Week 4/data\"\n",
        "\n",
        "    chatbot = EnhancedChatbot(MODEL_PATH, TOKENIZER_PATH, DATA_PATH)\n",
        "    chatbot.chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vk-ay0N72Km8",
        "outputId": "426bc713-cd5a-488a-d499-a2e1cc5e0ba7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "welcome! Type 'exit' to end the chat.\n",
            "You: i need cheap restaurant \n",
            "assistant: I found 22 matching restaurant(s):\n",
            "\n",
            "1. pizza hut city centre (restaurant) in centre - cheap price range\n",
            "   phone: 01223323737\n",
            "   address: Regent Street City Centre\n",
            "2. the missing sock (restaurant) in east - cheap price range\n",
            "   phone: 01223812660\n",
            "   address: Finders Corner Newmarket Road\n",
            "3. charlie chan (restaurant) in centre - cheap price range\n",
            "   phone: 01223361763\n",
            "   address: Regent Street City Centre\n",
            "4. ask restaurant (restaurant) in centre - cheap price range\n",
            "   phone: 01223364917\n",
            "   address: 12 Bridge Street City Centre\n",
            "5. kohinoor (restaurant) in centre - cheap price range\n",
            "   phone: 01223323639\n",
            "   address: 74 Mill Road City Centre\n",
            "6. rice house (restaurant) in centre - cheap price range\n",
            "   phone: 01223367755\n",
            "   address: 88 Mill Road City Centre\n",
            "7. thanh binh (restaurant) in west - cheap price range\n",
            "   phone: 01223362456\n",
            "   address: 17 Magdalene Street City Centre\n",
            "8. da vinci pizzeria (restaurant) in north - cheap price range\n",
            "   phone: 01223351707\n",
            "   address: 20 Milton Road Chesterton\n",
            "9. nandos (restaurant) in south - cheap price range\n",
            "   phone: 01223327908\n",
            "   address: Cambridge Leisure Park Clifton Way\n",
            "10. the lucky star (restaurant) in south - cheap price range\n",
            "   phone: 01223244277\n",
            "   address: Cambridge Leisure Park Clifton Way Cherry Hinton\n",
            "11. dojo noodle bar (restaurant) in centre - cheap price range\n",
            "   phone: 01223363471\n",
            "   address: 40210 Millers Yard City Centre\n",
            "12. la margherita (restaurant) in west - cheap price range\n",
            "   phone: 01223315232\n",
            "   address: 15 Magdalene Street City Centre\n",
            "13. golden house (restaurant) in centre - cheap price range\n",
            "   phone: 01842753771\n",
            "   address: 12 Lensfield Road City Centre\n",
            "14. j restaurant (restaurant) in centre - cheap price range\n",
            "   phone: 01223307581\n",
            "   address: 86 Regent Street City Centre\n",
            "15. the gardenia (restaurant) in centre - cheap price range\n",
            "   phone: 01223356354\n",
            "   address: 2 Rose Crescent City Centre\n",
            "16. zizzi cambridge (restaurant) in centre - cheap price range\n",
            "   phone: 01223365599\n",
            "   address: 47-53 Regent Street\n",
            "17. nandos city centre (restaurant) in centre - cheap price range\n",
            "   phone: 01223327908\n",
            "   address: 33-34 Saint Andrews Street\n",
            "18. royal spice (restaurant) in north - cheap price range\n",
            "   phone: 01733553355\n",
            "   address: Victoria Avenue Chesterton\n",
            "19. la raza (restaurant) in centre - cheap price range\n",
            "   phone: 01223464550\n",
            "   address: 4 - 6 Rose Crescent\n",
            "20. the gandhi (restaurant) in centre - cheap price range\n",
            "   phone: 01223353942\n",
            "   address: 72 Regent Street City Centre\n",
            "21. mahal of cambridge (restaurant) in centre - cheap price range\n",
            "   phone: 01223360409\n",
            "   address: 3 - 5 Millers Yard Mill Lane\n",
            "22. the river bar steakhouse and grill (restaurant) in centre - cheap price range\n",
            "   phone: 01223307030\n",
            "   address: Quayside Off Bridge Street\n",
            "\n",
            "Chat ended by user.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}